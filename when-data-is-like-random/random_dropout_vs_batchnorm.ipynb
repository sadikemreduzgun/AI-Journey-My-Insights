{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d685c0a-5889-4795-8b48-c22384fcf95d",
   "metadata": {},
   "source": [
    "### Functional API in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748974b2-4025-4f5b-b048-83f81f51df0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install TensorFlow\n",
    "!pip install tensorflow==2.16.2\n",
    "\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.layers import Input, Dense \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='tensorflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1ae7cd-e00f-4cf2-bbd8-4ceb0309f576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "X_train = np.random.rand(1000, 20) \n",
    "y_train = np.random.randint(2, size=(1000, 1)) \n",
    "\n",
    "X_test = np.random.rand(200, 20) \n",
    "y_test = np.random.randint(2, size=(200, 1)) \n",
    "loss, accuracy = model.evaluate(X_test, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03df38ab-7fa3-4e4b-a3fd-bb369cf36df1",
   "metadata": {},
   "source": [
    "### Dropout and Batch Normalization\n",
    "\n",
    "Before we proceed with the practice exercise, let's briefly discuss two important techniques often used to improve the performance of neural networks: **Dropout Layers** and **Batch Normalization**.\n",
    "\n",
    "#### Dropout Layers\n",
    "\n",
    "Dropout is a regularization technique that helps prevent overfitting in neural networks. During training, Dropout randomly sets a fraction of input units to zero at each update cycle. This prevents the model from becoming overly reliant on any specific neurons, which encourages the network to learn more robust features that generalize better to unseen data.\n",
    "\n",
    "**Key points:**\n",
    "- Dropout is only applied during training, not during inference.\n",
    "- The dropout rate is a hyperparameter that determines the fraction of neurons to drop.\n",
    "\n",
    "\n",
    "#### Batch Normalization\n",
    "\n",
    "Batch Normalization is a technique used to improve the training stability and speed of neural networks. It normalizes the output of a previous layer by re-centering and re-scaling the data, which helps in stabilizing the learning process. By reducing the internal covariate shift (the changes in the distribution of layer inputs), batch normalization allows the model to use higher learning rates, which often speeds up convergence.\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "- Batch normalization works by normalizing the inputs to each layer to have a mean of zero and a variance of one.\n",
    "- It is applied during both training and inference, although its behavior varies slightly between the two phases.\n",
    "- Batch normalization layers also introduce two learnable parameters that allow the model to scale and - shift the normalized output, which helps in restoring the model's representational power.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57026c45-14c1-4c1b-abc4-d635cbb2a3be",
   "metadata": {},
   "source": [
    "### Run Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2563dc9-f841-488e-b80c-84e3b2e87b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout, Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "def get_accs(run):\n",
    "    accs = 0\n",
    "    for i in range(run):\n",
    "        # Define the input layer\n",
    "        input_layer = Input(shape=(20,))\n",
    "        \n",
    "        # Add hidden layers with dropout\n",
    "        hidden_layer1 = Dense(64, activation='relu')(input_layer)\n",
    "        dropout1 = Dropout(0.5)(hidden_layer1)\n",
    "        hidden_layer2 = Dense(64, activation='relu')(dropout1)\n",
    "        dropout2 = Dropout(0.5)(hidden_layer2)\n",
    "        \n",
    "        # Define the output layer\n",
    "        output_layer = Dense(1, activation='sigmoid')(dropout2)\n",
    "        \n",
    "        # Create the model\n",
    "        model = Model(inputs=input_layer, outputs=output_layer)\n",
    "        model.summary()\n",
    "        \n",
    "        # Compile the model\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        loss, accuracy = model.evaluate(X_test, y_test)\n",
    "        #print(f'Test loss: {loss}')\n",
    "        #print(f'Test accuracy: {accuracy}')\n",
    "        accs += accuracy\n",
    "\n",
    "    print(accs/run)\n",
    "\n",
    "get_accs(10)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a976923-3976-4c63-bdcb-e77a0f60f2bc",
   "metadata": {},
   "source": [
    "### Run Batch Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d44e256-5fb1-4493-86a4-f02dd0d61f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "\n",
    "def runn(run):\n",
    "\n",
    "    accs = 0\n",
    "\n",
    "    for i in range(run):\n",
    "        # Define the input layer\n",
    "        input_layer = Input(shape=(20,))\n",
    "        \n",
    "        # Add hidden layers with batch normalization\n",
    "        hidden_layer1 = Dense(64, activation='relu')(input_layer)\n",
    "        batch_norm1 = BatchNormalization()(hidden_layer1)\n",
    "        hidden_layer2 = Dense(64, activation='relu')(batch_norm1)\n",
    "        batch_norm2 = BatchNormalization()(hidden_layer2)\n",
    "        \n",
    "        # Define the output layer\n",
    "        output_layer = Dense(1, activation='sigmoid')(batch_norm2)\n",
    "        \n",
    "        # Create the model\n",
    "        model = Model(inputs=input_layer, outputs=output_layer)\n",
    "        model.summary()\n",
    "        \n",
    "        # Compile the model\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "        accs += accuracy\n",
    "\n",
    "    print(accs/run)\n",
    "\n",
    "runn(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073a672f-2543-47eb-abac-0f3b3f696338",
   "metadata": {},
   "source": [
    "Copyright Â© IBM Corporation. All rights reserved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae89cbc5-d43b-4380-a2e9-82d76745ca05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
